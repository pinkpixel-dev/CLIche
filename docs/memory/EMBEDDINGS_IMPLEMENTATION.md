# CLIche Embeddings Implementation

## Overview

This document details the implementation of the new embeddings system for CLIche, based on mem0's architecture. The system will support multiple embedding providers, automatic model downloading, and proper handling of different embedding dimensions.

## Core Components

### 1. Base Embedding Provider

```python
# cliche/memory/embeddings/base.py

from abc import ABC, abstractmethod
from typing import List, Optional, Literal, Dict, Any

class BaseEmbeddingConfig:
    """Base configuration class for embedding providers"""
    
    def __init__(self, **kwargs):
        self.model = kwargs.get("model")
        self.api_key = kwargs.get("api_key")
        self.dimensions = kwargs.get("dimensions")
        self.base_url = kwargs.get("base_url")
        # Other common configuration options
        
        # Provider-specific configuration
        for key, value in kwargs.items():
            if not hasattr(self, key):
                setattr(self, key, value)

class BaseEmbeddingProvider(ABC):
    """Base class for embedding providers"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize embedding provider with configuration
        
        Args:
            config: Configuration dictionary or None for defaults
        """
        if config is None:
            config = {}
        self.config = BaseEmbeddingConfig(**config)
        self.dimensions = getattr(self.config, "dimensions", None)
        
    @abstractmethod
    def embed(self, text: str, action: Optional[Literal["add", "search", "update"]] = None) -> List[float]:
        """
        Generate embedding vector for text
        
        Args:
            text: Text to embed
            action: Purpose of embedding (add, search, update)
            
        Returns:
            Embedding vector as list of floats
        """
        pass
    
    @abstractmethod
    def download_model_if_needed(self) -> bool:
        """
        Check if model is available and download if needed
        
        Returns:
            True if model is available or successfully downloaded
        """
        pass
    
    def get_dimensions(self) -> int:
        """
        Get the dimensions of embeddings generated by this provider
        
        Returns:
            Number of dimensions or None if unknown
        """
        return self.dimensions
```

### 2. Ollama Embedding Provider

```python
# cliche/memory/embeddings/ollama.py

import os
import logging
import subprocess
import sys
from typing import List, Optional, Literal, Dict, Any

from .base import BaseEmbeddingProvider, BaseEmbeddingConfig

# Try importing ollama
try:
    import ollama
except ImportError:
    # Ask if user wants to install
    answer = input("Ollama Python client is required but not installed. Install now? (y/n): ")
    if answer.lower() == "y":
        subprocess.check_call([sys.executable, "-m", "pip", "install", "ollama"])
        import ollama
    else:
        print("Cannot continue without ollama package.")
        sys.exit(1)

logger = logging.getLogger(__name__)

class OllamaEmbeddingConfig(BaseEmbeddingConfig):
    """Configuration for Ollama embedding provider"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = self.model or "nomic-embed-text"
        self.base_url = self.base_url or "http://localhost:11434"
        
        # Model dimensions based on known models
        model_dimensions = {
            "nomic-embed-text": 768,
            "mxbai-embed-large": 1024,
            "mxbai-embed-small": 384,
            "all-minilm": 384,
            "e5-large-v2": 1024,
        }
        
        # Set dimensions based on model if not explicitly set
        if not self.dimensions and self.model in model_dimensions:
            self.dimensions = model_dimensions[self.model]
        elif not self.dimensions:
            # Default dimensions
            self.dimensions = 768

class OllamaEmbeddingProvider(BaseEmbeddingProvider):
    """Ollama embedding provider implementation"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize Ollama embedding provider
        
        Args:
            config: Configuration dictionary or None for defaults
        """
        if config is None:
            config = {}
        self.config = OllamaEmbeddingConfig(**config)
        self.dimensions = self.config.dimensions
        
        # Initialize Ollama client
        self.client = ollama.Client(host=self.config.base_url)
        
    def embed(self, text: str, action: Optional[Literal["add", "search", "update"]] = None) -> List[float]:
        """
        Generate embedding vector using Ollama
        
        Args:
            text: Text to embed
            action: Purpose of embedding (add, search, update)
            
        Returns:
            Embedding vector as list of floats
        """
        try:
            # Ensure model is available
            self.download_model_if_needed()
            
            # Get embedding from Ollama
            response = self.client.embeddings(model=self.config.model, prompt=text)
            
            # Return the embedding vector
            return response.get("embedding", [])
        except Exception as e:
            logger.error(f"Error generating embedding with Ollama: {str(e)}")
            # Return empty vector or raise based on your error handling strategy
            raise
    
    def download_model_if_needed(self) -> bool:
        """
        Check if model is available and download if needed
        
        Returns:
            True if model is available or successfully downloaded
        """
        try:
            # List available models
            models = self.client.list().get("models", [])
            model_names = [model.get("name") for model in models]
            
            # Check if our model is in the list
            if self.config.model not in model_names:
                logger.info(f"Downloading Ollama model: {self.config.model}")
                self.client.pull(self.config.model)
                logger.info(f"Successfully downloaded model: {self.config.model}")
            
            return True
        except Exception as e:
            logger.error(f"Error downloading model: {str(e)}")
            return False
```

### 3. OpenAI Embedding Provider

```python
# cliche/memory/embeddings/openai.py

import os
import logging
from typing import List, Optional, Literal, Dict, Any

from .base import BaseEmbeddingProvider, BaseEmbeddingConfig

# Try importing openai
try:
    import openai
    from openai import OpenAI
except ImportError:
    raise ImportError("OpenAI Python client is required but not installed. Install with: pip install openai")

logger = logging.getLogger(__name__)

class OpenAIEmbeddingConfig(BaseEmbeddingConfig):
    """Configuration for OpenAI embedding provider"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = self.model or "text-embedding-3-small"
        self.api_key = self.api_key or os.environ.get("OPENAI_API_KEY")
        
        # Model dimensions based on OpenAI models
        model_dimensions = {
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
            "text-embedding-ada-002": 1536
        }
        
        # Set dimensions based on model
        if not self.dimensions and self.model in model_dimensions:
            self.dimensions = model_dimensions[self.model]
        elif not self.dimensions:
            # Default dimensions for OpenAI
            self.dimensions = 1536

class OpenAIEmbeddingProvider(BaseEmbeddingProvider):
    """OpenAI embedding provider implementation"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize OpenAI embedding provider
        
        Args:
            config: Configuration dictionary or None for defaults
        """
        if config is None:
            config = {}
        self.config = OpenAIEmbeddingConfig(**config)
        self.dimensions = self.config.dimensions
        
        # Check for API key
        if not self.config.api_key:
            raise ValueError("OpenAI API key is required")
        
        # Initialize OpenAI client
        self.client = OpenAI(api_key=self.config.api_key)
        
    def embed(self, text: str, action: Optional[Literal["add", "search", "update"]] = None) -> List[float]:
        """
        Generate embedding vector using OpenAI
        
        Args:
            text: Text to embed
            action: Purpose of embedding (add, search, update)
            
        Returns:
            Embedding vector as list of floats
        """
        try:
            # Get embedding from OpenAI
            response = self.client.embeddings.create(
                input=text,
                model=self.config.model
            )
            
            # Return the embedding vector
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating embedding with OpenAI: {str(e)}")
            # Return empty vector or raise based on your error handling strategy
            raise
    
    def download_model_if_needed(self) -> bool:
        """
        No need to download models for API-based providers
        
        Returns:
            Always True for API providers
        """
        return True
```

### 4. Anthropic Embedding Provider

```python
# cliche/memory/embeddings/anthropic.py

import os
import logging
from typing import List, Optional, Literal, Dict, Any

from .base import BaseEmbeddingProvider, BaseEmbeddingConfig

# Try importing anthropic
try:
    import anthropic
except ImportError:
    raise ImportError("Anthropic Python client is required but not installed. Install with: pip install anthropic")

logger = logging.getLogger(__name__)

class AnthropicEmbeddingConfig(BaseEmbeddingConfig):
    """Configuration for Anthropic embedding provider"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = self.model or "claude-3-haiku-20240307"
        self.api_key = self.api_key or os.environ.get("ANTHROPIC_API_KEY")
        
        # Default dimensions for Anthropic (this may change as they release official embedding models)
        self.dimensions = self.dimensions or 1024

class AnthropicEmbeddingProvider(BaseEmbeddingProvider):
    """Anthropic embedding provider implementation"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize Anthropic embedding provider
        
        Args:
            config: Configuration dictionary or None for defaults
        """
        if config is None:
            config = {}
        self.config = AnthropicEmbeddingConfig(**config)
        self.dimensions = self.config.dimensions
        
        # Check for API key
        if not self.config.api_key:
            raise ValueError("Anthropic API key is required")
        
        # Initialize Anthropic client
        self.client = anthropic.Anthropic(api_key=self.config.api_key)
        
    def embed(self, text: str, action: Optional[Literal["add", "search", "update"]] = None) -> List[float]:
        """
        Generate embedding vector using Anthropic
        
        Args:
            text: Text to embed
            action: Purpose of embedding (add, search, update)
            
        Returns:
            Embedding vector as list of floats
        """
        try:
            # NOTE: As of this writing, Anthropic doesn't have an embedding-specific API
            # This is a placeholder for when they release one
            # For now, we would need to use a different provider for embeddings
            
            # Placeholder implementation - replace when Anthropic has embeddings API
            raise NotImplementedError("Anthropic does not currently offer an embeddings API")
        except Exception as e:
            logger.error(f"Error generating embedding with Anthropic: {str(e)}")
            raise
    
    def download_model_if_needed(self) -> bool:
        """
        No need to download models for API-based providers
        
        Returns:
            Always True for API providers
        """
        return True
```

### 5. Factory Implementation

```python
# cliche/memory/embeddings/factory.py

from typing import Dict, Any, Optional
import logging

from .base import BaseEmbeddingProvider
from .ollama import OllamaEmbeddingProvider
from .openai import OpenAIEmbeddingProvider
from .anthropic import AnthropicEmbeddingProvider

logger = logging.getLogger(__name__)

class EmbeddingProviderFactory:
    """Factory for creating embedding providers"""
    
    # Map of provider names to provider classes
    PROVIDERS = {
        "ollama": OllamaEmbeddingProvider,
        "openai": OpenAIEmbeddingProvider,
        "anthropic": AnthropicEmbeddingProvider,
        # Add more providers as implemented
    }
    
    @classmethod
    def create(cls, provider_name: str, config: Optional[Dict[str, Any]] = None) -> BaseEmbeddingProvider:
        """
        Create an embedding provider instance
        
        Args:
            provider_name: Name of the provider to create
            config: Configuration dictionary or None for defaults
            
        Returns:
            Embedding provider instance
            
        Raises:
            ValueError: If provider_name is not supported
        """
        if provider_name not in cls.PROVIDERS:
            supported = ", ".join(cls.PROVIDERS.keys())
            raise ValueError(f"Unsupported embedding provider: {provider_name}. Supported providers: {supported}")
        
        # Create and return provider instance
        provider_class = cls.PROVIDERS[provider_name]
        try:
            return provider_class(config)
        except Exception as e:
            logger.error(f"Error creating embedding provider {provider_name}: {str(e)}")
            raise
```

## Usage in Memory System

The embedding provider system will be used in the memory system as follows:

```python
# Example usage in memory/main.py

from .embeddings.factory import EmbeddingProviderFactory
from .vector_stores.factory import VectorStoreFactory

class Memory:
    def __init__(self, config=None):
        if config is None:
            config = {}
        
        # Get embedding provider configuration
        embedding_config = config.get("embedding", {})
        provider_name = embedding_config.get("provider", "ollama")
        
        # Create embedding provider
        self.embedding_provider = EmbeddingProviderFactory.create(
            provider_name, 
            embedding_config.get("config", {})
        )
        
        # Create vector store with dimensions from embedding provider
        vector_store_config = config.get("vector_store", {})
        vector_store_config["config"] = vector_store_config.get("config", {})
        vector_store_config["config"]["dimensions"] = self.embedding_provider.get_dimensions()
        
        self.vector_store = VectorStoreFactory.create(
            vector_store_config.get("provider", "chroma"),
            vector_store_config.get("config", {})
        )
    
    def add(self, content, metadata=None):
        """Add a memory with proper embedding"""
        # Generate embedding
        embedding = self.embedding_provider.embed(content, action="add")
        
        # Store in vector store
        return self.vector_store.add(content, embedding, metadata)
    
    def search(self, query, limit=5):
        """Search for memories similar to query"""
        # Generate embedding
        query_embedding = self.embedding_provider.embed(query, action="search")
        
        # Search vector store
        return self.vector_store.search(query_embedding, limit=limit)
```

## CLI Implementation

```python
# Example CLI implementation in commands/memory.py

@memory.command()
@click.option('--model', '-m', help="Specify the embedding model to install")
@click.option('--provider', '-p', help="Specify the embedding provider to use")
def install(model, provider):
    """Install embedding model for memory system"""
    assistant = get_cliche_instance()
    
    # Get current provider or use specified one
    current_provider = assistant.memory.embedding_provider.__class__.__name__
    provider_name = provider or current_provider
    
    # Create config
    config = {}
    if model:
        config["model"] = model
    
    # Create provider instance
    try:
        provider = EmbeddingProviderFactory.create(provider_name, config)
        
        # Download model if needed
        click.echo(f"Checking for model: {provider.config.model}")
        success = provider.download_model_if_needed()
        
        if success:
            click.echo(f"Model {provider.config.model} is ready to use")
        else:
            click.echo(f"Failed to download model {provider.config.model}")
    except Exception as e:
        click.echo(f"Error: {str(e)}")
```

## Model Size Considerations

### Small vs. Large Models

The choice between small and large embedding models involves several tradeoffs:

| Factor | Small Models | Large Models |
|--------|-------------|--------------|
| Embedding Quality | Good | Excellent |
| Speed | Fast | Slower |
| Disk Space | 300MB-1GB | 1GB-5GB |
| RAM Usage | Lower | Higher |
| Accuracy for Similar Concepts | Good | Excellent |

When deciding which model to use, consider:

1. **Hardware Constraints**: On machines with limited RAM or disk space, smaller models are preferable.
2. **Response Time Needs**: If quick responses are critical, smaller models provide faster embedding generation.
3. **Accuracy Requirements**: For applications where subtle semantic differences are important, larger models excel.

### Recommendations

For CLIche, we'll offer both options with smart defaults:

1. **Default**: Use `nomic-embed-text` (medium size, good quality) for most users
2. **Small Option**: Offer `all-minilm` or `mxbai-embed-small` for resource-constrained environments
3. **Large Option**: Provide `mxbai-embed-large` for users who prioritize accuracy

Using the larger model makes a significant difference in:
- Nuanced concept understanding
- Cross-language similarity
- Domain-specific terminology
- Long text embeddings

However, the smaller models are entirely sufficient for:
- Basic fact retrieval
- Simple preference tracking
- Short text similarity

## Implementation Timeline

1. **Base Components**: 1-2 days
   - Implement base classes and interfaces
   - Create factory implementation

2. **Provider Implementations**: 2-3 days
   - Implement Ollama provider (highest priority)
   - Implement OpenAI provider
   - Implement additional providers

3. **Integration**: 1-2 days
   - Connect with vector stores
   - Update memory system to use the new providers

4. **CLI and Configuration**: 1 day
   - Implement CLI commands for model management
   - Create configuration options for embedding providers

5. **Testing and Refinement**: 1-2 days
   - Test with different providers and models
   - Optimize performance
   - Handle edge cases 