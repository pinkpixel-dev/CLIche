"""
Web research command for CLIche.
Enables research with up-to-date web content.
"""
import os
import json
import asyncio
import click
import re
import inspect
from pathlib import Path
from rich.console import Console
from ..core import cli, CLIche, get_llm
from ..utils.file import save_text_to_file, get_docs_dir, get_unique_filename
from ..utils.unsplash import UnsplashAPI, format_image_for_markdown, format_image_for_html, get_photo_credit

# Initialize console for rich output
console = Console()

# Check if search packages are available
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS = None
    DDGS_AVAILABLE = False

# Check if the web crawler package is available
try:
    from crawl4ai import AsyncWebCrawler
    
    # Inspect the methods available in AsyncWebCrawler
    # This will help us determine the correct method to use
    CRAWLER_METHODS = [method for method in dir(AsyncWebCrawler) 
                      if not method.startswith('_') and callable(getattr(AsyncWebCrawler, method, None))]
    
    try:
        # Try to import CrawlerRunConfig - it might have different name in different versions
        from crawl4ai import CrawlerRunConfig
    except ImportError:
        # Fallback to a simple config class if not available
        class CrawlerRunConfig:
            def __init__(self, **kwargs):
                for key, value in kwargs.items():
                    setattr(self, key, value)
except ImportError:
    AsyncWebCrawler = None
    CRAWLER_METHODS = []

# Check if requests is available for Brave Search API
try:
    import requests
    import json
    BRAVE_SEARCH_AVAILABLE = True
except ImportError:
    BRAVE_SEARCH_AVAILABLE = False

# Check if requests package is available (for fallback scraping)
try:
    from bs4 import BeautifulSoup
    FALLBACK_SCRAPER_AVAILABLE = True
except ImportError:
    FALLBACK_SCRAPER_AVAILABLE = False

def perform_search(query, num_results=5, search_engine='auto'):
    """Perform a search and return the top results.
    
    Args:
        query: Search query string
        num_results: Maximum number of results to return
        search_engine: Which search engine to use ('auto', 'duckduckgo', or 'brave')
        
    Will try multiple search providers based on the search_engine parameter.
    - 'auto': Try all available providers in sequence
    - 'duckduckgo': Only use DuckDuckGo 
    - 'brave': Only use Brave Search
    """
    results = []
    
    # Try DuckDuckGo first if auto or duckduckgo explicitly requested
    if DDGS_AVAILABLE and search_engine in ['auto', 'duckduckgo']:
        try:
            console.print("üîç Searching with DuckDuckGo...")
            with DDGS() as ddgs:
                for r in ddgs.text(query, max_results=num_results):
                    # Ensure we have a valid URL and title
                    url = r.get('href', '')
                    title = r.get('title', '')
                    
                    if not url or not title:
                        continue
                        
                    results.append({
                        'link': url,
                        'title': title,
                        'snippet': r.get('body', '')
                    })
            
            if results:
                console.print(f"‚úÖ Found {len(results)} results with DuckDuckGo")
                return results
            else:
                console.print("‚ö†Ô∏è No results found with DuckDuckGo. Trying alternative...")
        except Exception as e:
            ddg_error = str(e)
            console.print(f"‚ö†Ô∏è DuckDuckGo search error: {ddg_error}")
            
            # Don't fall through to fallback if it's not a rate limit
            if not ("ratelimit" in ddg_error.lower() or "rate limit" in ddg_error.lower()):
                console.print("‚ö†Ô∏è Trying alternative search provider...")
    else:
        console.print("‚ö†Ô∏è DuckDuckGo search package not installed. Trying alternative...")
    
    # Try Brave Search API if auto or brave explicitly requested
    if search_engine in ['auto', 'brave']:
        brave_results = brave_search(query, num_results)
        if brave_results:
            console.print(f"‚úÖ Found {len(brave_results)} results with Brave Search")
            results.extend(brave_results)
            return results
    
    # If only brave was requested but it failed, log specific message
    if search_engine == 'brave' and not results:
        console.print("‚ùå Brave Search failed to return results. Check your API key or try another search engine.")
    
    # If we got here, both search providers failed or returned no results
    if not results:
        console.print("‚ö†Ô∏è All search providers failed. Using fallback test data.")
        
        # Create mock search results for testing
        fallback_results = []
        
        # Add some common websites based on the query term
        query_terms = query.lower().split()
        
        # Python-related queries
        if any(term in query_terms for term in ["python", "programming", "code", "developer"]):
            fallback_results.extend([
                {
                    'link': 'https://docs.python.org/3/tutorial/index.html',
                    'title': 'The Python Tutorial',
                    'snippet': 'Python is an easy to learn, powerful programming language...'
                },
                {
                    'link': 'https://realpython.com/tutorials/advanced/',
                    'title': 'Advanced Python Tutorials ‚Äì Real Python',
                    'snippet': 'Advanced Python tutorials to help you level up your Python skills...'
                },
                {
                    'link': 'https://www.geeksforgeeks.org/python-programming-language/',
                    'title': 'Python Programming Language - GeeksforGeeks',
                    'snippet': 'Python is a high-level, general-purpose and very popular programming language...'
                }
            ])
        
        # Quantum computing related queries
        if any(term in query_terms for term in ["quantum", "computing", "physics"]):
            fallback_results.extend([
                {
                    'link': 'https://www.ibm.com/quantum/what-is-quantum-computing',
                    'title': 'What is quantum computing? | IBM',
                    'snippet': 'Quantum computing harnesses quantum mechanical phenomena to create powerful quantum computers...'
                },
                {
                    'link': 'https://en.wikipedia.org/wiki/Quantum_computing',
                    'title': 'Quantum computing - Wikipedia',
                    'snippet': 'Quantum computing is a type of computation whose operations can harness the phenomena of quantum mechanics...'
                },
                {
                    'link': 'https://aws.amazon.com/what-is/quantum-computing/',
                    'title': 'What is Quantum Computing? - AWS',
                    'snippet': 'Quantum computing is an area of computer science that uses quantum mechanics principles...'
                }
            ])
        
        # AI and Machine Learning
        if any(term in query_terms for term in ["ai", "ml", "artificial", "intelligence", "machine", "learning"]):
            fallback_results.extend([
                {
                    'link': 'https://www.ibm.com/topics/artificial-intelligence',
                    'title': 'What is Artificial Intelligence (AI)? | IBM',
                    'snippet': 'Artificial intelligence is a field of computer science dedicated to solving cognitive problems...'
                },
                {
                    'link': 'https://en.wikipedia.org/wiki/Machine_learning',
                    'title': 'Machine learning - Wikipedia',
                    'snippet': 'Machine learning is a field of study in artificial intelligence concerned with the development...'
                },
                {
                    'link': 'https://www.coursera.org/specializations/machine-learning-introduction',
                    'title': 'Machine Learning Specialization - Coursera',
                    'snippet': 'The Machine Learning Specialization is a foundational online program created in collaboration...'
                }
            ])
        
        # General technology fallback if nothing specific matches
        if not fallback_results:
            fallback_results.extend([
                {
                    'link': 'https://en.wikipedia.org/wiki/' + query.replace(' ', '_'),
                    'title': query + ' - Wikipedia',
                    'snippet': 'Overview and introduction to ' + query + ' including history, applications, and current developments...'
                },
                {
                    'link': 'https://www.investopedia.com/terms/' + query.replace(' ', '-').lower(),
                    'title': 'What Is ' + query + '? Definition and Examples',
                    'snippet': query + ' refers to technology, methods, and practices used in various domains...'
                }
            ])
        
        # Limit results to num_results
        return fallback_results[:num_results]
    
    return results

def extract_text_from_html(html_content):
    """Simple fallback function to extract text from HTML."""
    try:
        if not FALLBACK_SCRAPER_AVAILABLE:
            return "Fallback scraper not available. Please install beautifulsoup4."
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()
            
        # Get text
        text = soup.get_text()
        
        # Break into lines and remove leading and trailing spaces
        lines = (line.strip() for line in text.splitlines())
        
        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        
        # Drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        return f"Error extracting text: {str(e)}"

async def fallback_scrape(url, debug=False):
    """Fallback scraping method using requests and BeautifulSoup."""
    try:
        if not FALLBACK_SCRAPER_AVAILABLE:
            if debug:
                click.echo("  Fallback scraper not available. Need beautifulsoup4.")
            return None
            
        if debug:
            click.echo(f"  Using fallback scraper on {url}")
            
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        html_content = response.text
        text_content = extract_text_from_html(html_content)
        
        if debug:
            click.echo(f"  Extracted {len(text_content)} chars with fallback scraper")
            
        return text_content
    except Exception as e:
        if debug:
            click.echo(f"  Fallback scraper error: {str(e)}")
        return None

async def extract_content_with_crawler(crawler, url, config, debug=False):
    """Try various methods to extract content with the crawler."""
    if debug:
        click.echo(f"  Available crawler methods: {CRAWLER_METHODS}")
    
    # Check if various methods are available and try them
    content = None
    extracted_text = None
    
    # Try 'arun' method - primary method in current crawl4ai versions
    if 'arun' in CRAWLER_METHODS:
        try:
            if debug:
                click.echo("  Trying crawler.arun() method")
            content = await crawler.arun(url, config)
            if debug:
                click.echo(f"  arun() returned type: {type(content)}")
                if content:
                    click.echo(f"  Content sample: {str(content)[:100]}...")
        except Exception as e:
            if debug:
                click.echo(f"  arun() method failed: {str(e)}")
    
    # Try 'aprocess_html' method - might be useful for pre-fetched HTML
    if content is None and 'aprocess_html' in CRAWLER_METHODS:
        try:
            if debug:
                click.echo("  Trying to fetch page content for aprocess_html")
            # First get the raw HTML
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    html = await response.text()
            if debug:
                click.echo(f"  Fetched {len(html)} bytes of HTML")
    
            if debug:
                click.echo("  Trying crawler.aprocess_html() method")
            content = await crawler.aprocess_html(html, url, config)
            if debug:
                click.echo(f"  aprocess_html() returned type: {type(content)}")
        except Exception as e:
            if debug:
                click.echo(f"  aprocess_html() method failed: {str(e)}")
    
    # Try to extract text from content object
    if content is not None:
        # Try various attributes that might contain text
        for attr in ['text', 'cleaned_text', 'content', 'cleaned_html', 'html']:
            if hasattr(content, attr):
                text_value = getattr(content, attr)
                if text_value and isinstance(text_value, str) and len(text_value) > 100:
                    extracted_text = text_value[:20000]  # Limit content size
                    if debug:
                        click.echo(f"  Found text in content.{attr}: {len(extracted_text)} chars")
                    break
    
    # If no text found, try using content directly if it's a string
    if extracted_text is None and content is not None and isinstance(content, str):
        try:
            if len(content) > 100:
                if '<html' in content.lower():
                    extracted_text = extract_text_from_html(content)[:20000]
                else:
                    extracted_text = content[:20000]  # Use directly if it's already plain text
                if debug:
                    click.echo(f"  Extracted {len(extracted_text)} chars from content string")
        except Exception as e:
            if debug:
                click.echo(f"  Error extracting text from content string: {str(e)}")
    
    # If content is a dictionary, check for common keys that might contain text
    if extracted_text is None and content is not None and isinstance(content, dict):
        for key in ['content', 'text', 'body', 'main', 'article']:
            if key in content and isinstance(content[key], str) and len(content[key]) > 100:
                extracted_text = content[key][:20000]
                if debug:
                    click.echo(f"  Found text in content['{key}']: {len(extracted_text)} chars")
                break
    
    return extracted_text

@cli.command()
@click.argument("query", nargs=-1)
@click.option("--depth", "-d", type=int, default=3, help="Number of search results to analyze")
@click.option("--debug", is_flag=True, help="Enable debug mode with detailed error messages")
@click.option("--fallback-only", is_flag=True, help="Skip primary crawler and use only the fallback scraper")
@click.option("--write", "-w", is_flag=True, help="Generate a document instead of terminal output")
@click.option("--format", "-f", type=click.Choice(['text', 'markdown', 'html']), default='markdown',
              help='Document format when using --write')
@click.option("--filename", help="Optional filename for the generated document")
@click.option("--image", "-i", help='Add images related to the topic by search term', type=str)
@click.option("--image-count", default=3, help='Number of images to add', type=int)
@click.option("--image-width", default=800, help='Width of images', type=int)
@click.option("--search-engine", type=click.Choice(['auto', 'duckduckgo', 'brave']), default='auto',
              help='Search engine to use (auto tries all available)')
@click.option("--summarize", is_flag=True, help="Generate a concise summary document instead of a comprehensive one")
@click.option("--snippet", is_flag=True, help="Generate a very brief snippet/overview (few paragraphs)")
def research(query, depth, debug, fallback_only, write, format, filename, 
             image, image_count, image_width, search_engine, summarize, snippet):
    """Research a topic online and generate a response or document.
    
    Research uses web search and content extraction to provide up-to-date information
    on any topic. The results are processed by an AI to create a comprehensive
    response or document.
    
    Examples:
      cliche research "Latest developments in quantum computing"
      cliche research "Python async programming best practices" -d 5
      cliche research "Climate change impacts" --write --format markdown
      cliche research "Mars exploration" -w -f markdown --image "mars rover" --image-count 2
      cliche research "Artificial intelligence" --write --summarize
      cliche research "Quantum physics" --snippet
    """
    
    # Join query parts
    query_str = ' '.join(query)
    
    if not query_str:
        console.print("[bold red]Please provide a search query.[/bold red]")
        console.print("Example: cliche research \"Latest developments in quantum computing\"")
        return
    
    console.print(f"[bold]Researching:[/bold] {query_str}")
    
    # Initialize image data dictionary
    image_data = {"images": [], "credits": []}
    
    # Fetch images if requested for writing mode
    if write and image:
        try:
            unsplash = UnsplashAPI()
            
            # Search for images by term
            console.print(f"üîç Searching for '{image}' images...")
            results = unsplash.search_photos(image, per_page=image_count)
            
            # Check if we have results
            photos = results.get('results', [])
            if not photos:
                console.print("‚ùå No images found for this search term.")
            else:
                # Download each image
                for i, photo in enumerate(photos[:image_count]):
                    photo_id = photo.get('id')
                    console.print(f"üñºÔ∏è Getting image {i+1}/{min(image_count, len(photos))}...")
                    
                    try:
                        photo_data = unsplash.get_photo_url(photo_id, width=image_width)
                        
                        # Add to image data
                        image_data["images"].append({
                            "url": photo_data["url"],
                            "alt_text": photo_data["alt_text"],
                            "width": image_width
                        })
                        image_data["credits"].append(get_photo_credit(photo, format))
                        
                    except Exception as e:
                        console.print(f"‚ö†Ô∏è Error getting image: {str(e)}")
            
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error fetching images: {str(e)}")
            console.print("Continuing without images...")
    
    # Check if required packages are installed
    if DDGS is None:
        click.echo("‚ùå DuckDuckGo search package not installed. Run: pip install duckduckgo-search")
        return
        
    if AsyncWebCrawler is None and not FALLBACK_SCRAPER_AVAILABLE:
        click.echo("‚ùå No web scrapers available. Run: pip install crawl4ai beautifulsoup4")
        return
        
    if debug:
        click.echo("üîç DEBUG MODE ENABLED")
        if AsyncWebCrawler:
            click.echo(f"Using crawl4ai with AsyncWebCrawler: {AsyncWebCrawler}")
            click.echo(f"Available crawler methods: {CRAWLER_METHODS}")
        if FALLBACK_SCRAPER_AVAILABLE:
            click.echo("Fallback scraper (BeautifulSoup) is available")
        if fallback_only:
            click.echo("Using fallback scraper only (primary crawler disabled)")
        if write:
            click.echo(f"Document generation enabled with format: {format}")
        
    console.print(f"üîç Researching: {query_str}...")

    # Perform a web search with the specified search engine
    search_results = perform_search(query_str, num_results=depth, search_engine=search_engine)

    if not search_results:
        click.echo("‚ùå No search results found.")
        return
    
    # Select top N results
    selected_results = search_results[:depth]
    
    extracted_data = []
    
    async def scrape_and_extract():
        # Only use crawler if available and not in fallback-only mode
        use_crawler = AsyncWebCrawler is not None and not fallback_only
        
        if use_crawler:
            try:
                async with AsyncWebCrawler() as crawler:
                    for result in selected_results:
                        url = result['link']
                        title = result['title']
                        
                        if not url:
                            continue
                        
                        console.print(f"üåê Scraping: {title}")
                        
                        try:
                            if debug:
                                console.print(f"  Creating crawler config for {url}")
                            
                            config = CrawlerRunConfig(
                                page_timeout=30000,
                                wait_until='load',
                                scan_full_page=True,
                                word_count_threshold=100
                            )
                            
                            extracted_text = await extract_content_with_crawler(crawler, url, config, debug)
                            
                            if extracted_text:
                                extracted_data.append({
                                    "title": title,
                                    "url": url,
                                    "content": extracted_text,
                                    "snippet": result.get('snippet', '')
                                })
                                console.print(f"‚úÖ Content extracted: {len(extracted_text)} chars")
                            else:
                                # Try fallback scraping
                                # Try alternate extraction method
                                fallback_content = await fallback_scrape(url, debug)
                                
                                if fallback_content and len(fallback_content) > 100:
                                    extracted_text = fallback_content[:20000]  # Increased content size limit
                                    extracted_data.append({
                                        "title": title,
                                        "url": url,
                                        "content": extracted_text,
                                        "snippet": result.get('snippet', '')
                                    })
                                    console.print(f"‚úÖ Extraction succeeded: {len(extracted_text)} chars")
                        except Exception as e:
                            error_msg = f"‚ùå Error scraping {url}: {str(e)}"
                            if debug:
                                import traceback
                                error_msg += f"\n{traceback.format_exc()}"
                            console.print(error_msg)
                            
                            # Always try fallback when crawler fails
                            try:
                                console.print(f"‚ö†Ô∏è Trying fallback scraper after error...")
                                fallback_content = await fallback_scrape(url, debug)
                                
                                if fallback_content and len(fallback_content) > 100:
                                    extracted_text = fallback_content[:20000]  # Increased content size limit
                                    extracted_data.append({
                                        "title": title,
                                        "url": url,
                                        "content": extracted_text,
                                        "snippet": result.get('snippet', '')
                                    })
                                    console.print(f"‚úÖ Extraction succeeded: {len(extracted_text)} chars")
                            except Exception as inner_e:
                                if debug:
                                    console.print(f"‚ö†Ô∏è Fallback scraper also failed: {str(inner_e)}")
            except Exception as e:
                error_msg = f"‚ùå Error initializing crawler: {str(e)}"
                if debug:
                    import traceback
                    error_msg += f"\n{traceback.format_exc()}"
                console.print(error_msg)
                console.print("‚ö†Ô∏è Falling back to simple scraper for all URLs")
        
        # If fallback-only mode or crawler failed completely, use fallback on all URLs
        if fallback_only or (not use_crawler) or (use_crawler and not extracted_data):
            for result in selected_results:
                url = result['link']
                title = result['title']
                
                if not url:
                    continue
                
                if not fallback_only:  # Only show this message if we're not intentionally using fallback only
                    console.print(f"üåê Fallback scraping: {title}")
                else:
                    console.print(f"üåê Scraping: {title}")
                    
                try:
                    # Try fallback scraping
                    fallback_content = await fallback_scrape(url, debug)
                    
                    if fallback_content and len(fallback_content) > 100:
                        extracted_text = fallback_content[:20000]  # Increased content size limit
                        extracted_data.append({
                            "title": title,
                            "url": url,
                            "content": extracted_text,
                            "snippet": result.get('snippet', '')
                        })
                        console.print(f"‚úÖ Extraction succeeded: {len(extracted_text)} chars")
                    else:
                        console.print(f"‚ö†Ô∏è No content extracted from: {title}")
                except Exception as e:
                    error_msg = f"‚ùå Error scraping {url}: {str(e)}"
                    if debug:
                        import traceback
                        error_msg += f"\n{traceback.format_exc()}"
                    console.print(error_msg)
    
    # Run the scraping
    asyncio.run(scrape_and_extract())
    
    if not extracted_data:
        console.print("‚ùå No content could be extracted from any sources.")
        return 1  # Return error code for better detection in test script
    
    try:
        # Instead of processing all sources at once, we'll chunk them
        all_extracted_data = extracted_data.copy()
        
        # Check if we're generating a snippet or summary (no chunking needed)
        if snippet or summarize:
            # For snippets or summaries, we don't need chunking
            # Combine a limited amount of data from all sources
            combined_sources_info = ""
            source_limit = 2000 if snippet else 5000  # Very limited for snippets
            
            for idx, data in enumerate(all_extracted_data, 1):
                excerpt_length = min(source_limit // len(all_extracted_data), len(data['content']))
                combined_sources_info += f"Source {idx}: {data['title']}\n"
                combined_sources_info += f"URL: {data['url']}\n"
                combined_sources_info += f"Content: {data['content'][:excerpt_length]}...\n\n"
            
            # Create the appropriate template based on format and mode
            if snippet:
                if format == 'markdown':
                    doc_template = """Create a VERY BRIEF SNIPPET (maximum 2-3 paragraphs) about this topic.
                    
                    The snippet should provide a quick overview that could fit in a preview card or executive summary.
                    Include only the most essential information - core definition, key points, and relevance.
                    
                    Keep the total length under 300 words. Use markdown formatting.
                    """
                elif format == 'html':
                    doc_template = """Create a VERY BRIEF SNIPPET (maximum 2-3 paragraphs) about this topic in HTML format. The ENTIRE content must use proper HTML tags, not Markdown.
                    
                    The snippet should provide a quick overview that could fit in a preview card or executive summary.
                    Include only the most essential information - core definition, key points, and relevance.
                    
                    Keep the total length under 300 words.

EXTREMELY IMPORTANT: You MUST use HTML tags for EVERYTHING and NEVER use Markdown syntax anywhere in your response.
For example:
- CORRECT: <h1>Main Heading</h1>
- INCORRECT: # Main Heading

- CORRECT: <p>This is a paragraph with <strong>bold text</strong>.</p>
- INCORRECT: This is a paragraph with **bold text**.

- CORRECT: <ul><li>List item</li></ul>
- INCORRECT: - List item

DO NOT EVER USE # FOR HEADINGS OR ** FOR BOLD TEXT OR - FOR LISTS. Always use proper HTML tags.

Every single piece of content must be enclosed in appropriate HTML tags. Do not mix HTML and Markdown syntax anywhere.
                    """
                else:
                    doc_template = """Create a VERY BRIEF SNIPPET (maximum 2-3 paragraphs) about this topic.
                    
                    The snippet should provide a quick overview that could fit in a preview card or executive summary.
                    Include only the most essential information - core definition, key points, and relevance.
                    
                    Keep the total length under 300 words.
                    """
            elif summarize:
                if format == 'markdown':
                    doc_template = """
                    Create a CONCISE SUMMARY document about this topic.
                    
                    The summary should be informative but significantly shorter than a comprehensive document.
                    Focus on providing:
                    - Clear definition and overview
                    - Key points and important aspects
                    - Basic background information
                    - Current relevance
                    
                    Keep the length moderate (around 800-1000 words). Use markdown formatting with appropriate headings.
                    
                    EXTREMELY IMPORTANT:
                    1. DO NOT start your response with ```markdown or any code fences
                    2. DO NOT enclose your entire response in code fences
                    3. Do not use any opening or closing fences in your response
                    """
                elif format == 'html':
                    doc_template = """Create a CONCISE SUMMARY document about this topic in HTML format. The ENTIRE content must use proper HTML tags, not Markdown.
                    
                    The summary should be informative but significantly shorter than a comprehensive document.
                    Focus on providing:
                    - Clear definition and overview
                    - Key points and important aspects
                    - Basic background information
                    - Current relevance
                    
                    Keep the length moderate (around 800-1000 words).

EXTREMELY IMPORTANT: You MUST use HTML tags for EVERYTHING and NEVER use Markdown syntax anywhere in your response.
For example:
- CORRECT: <h1>Main Heading</h1>
- INCORRECT: # Main Heading

- CORRECT: <p>This is a paragraph with <strong>bold text</strong>.</p>
- INCORRECT: This is a paragraph with **bold text**.

- CORRECT: <ul><li>List item</li></ul>
- INCORRECT: - List item

DO NOT EVER USE # FOR HEADINGS OR ** FOR BOLD TEXT OR - FOR LISTS. Always use proper HTML tags.

Every single piece of content must be enclosed in appropriate HTML tags. Do not mix HTML and Markdown syntax anywhere.
                    """
                else:
                    doc_template = """Create a CONCISE SUMMARY document about this topic.
                    
                    The summary should be informative but significantly shorter than a comprehensive document.
                    Focus on providing:
                    - Clear definition and overview
                    - Key points and important aspects
                    - Basic background information
                    - Current relevance
                    
                    Keep the length moderate (around 800-1000 words). Use clear paragraph breaks and section indicators.
                    """
            
            # Get image instructions if we have images
            image_instructions = ""
            if image_data["images"] and (format == 'markdown' or format == 'html'):
                # No need for special image placeholder instructions anymore
                image_instructions = "\n\nEXTREMELY IMPORTANT: Do NOT start your response with ```markdown or any code fences. Do NOT enclose your entire response in code fences."
            
            # Build the prompt with image instructions placed prominently
            prompt = f"""
            {doc_template}
            
            {image_instructions}
            
            EXTREMELY IMPORTANT: Do NOT start your response with ```markdown or any code fences. 
            Do NOT enclose your entire response in code fences.
            
            Topic: {query_str}
            
            Based on the following web research results, create a {('snippet' if snippet else 'summary')}:
            
            RESEARCH DATA:
            {combined_sources_info}
            """
            
            console.print(f"üß† Generating {'snippet' if snippet else 'summary'} for {query_str}...")
            
            # Get the LLM instance
            llm = get_llm()
            
            # Generate content in a single pass
            professional_mode = write  # Use professional mode when generating a document
            response = asyncio.run(llm.generate_response(prompt, professional_mode=professional_mode))
            
            # For markdown, ensure we have a good title
            if format == 'markdown' and not response.strip().startswith("# "):
                title = query_str.title()
                response = f"# {title}\n\n{response}"
        else:
            # Process in chunks of 2 sources at a time
            chunk_size = 2
            chunked_responses = []
            
            for chunk_start in range(0, len(all_extracted_data), chunk_size):
                chunk_end = min(chunk_start + chunk_size, len(all_extracted_data))
                chunk_data = all_extracted_data[chunk_start:chunk_end]
                
                sources_info = ""
                for idx, data in enumerate(chunk_data, chunk_start + 1):
                    sources_info += f"Source {idx}: {data['title']}\n"
                    sources_info += f"URL: {data['url']}\n"
                    sources_info += f"Content: {data['content'][:5000]}...\n\n"
                
                # Create prompt for document generation specific to this chunk
                if format == 'markdown':
                    if chunk_start == 0:  # First chunk includes introduction
                        doc_template = """
                        Create the FIRST PART of an extremely detailed, comprehensive markdown document about this topic.
            
                        Please focus on the INTRODUCTION and FIRST MAJOR SECTIONS of the topic, covering:
                        - Overview and definition of the topic
                        - Historical background and origins
                        - Core concepts and fundamentals
                        - Early developments and pioneers
                        
                        This is the FIRST CHUNK of a multi-part document, so focus on providing a strong foundation.
                        
                        Format with proper markdown headings (## for main sections, ### for subsections).
                        
                        EXTREMELY IMPORTANT:
                        1. DO NOT start your response with ```markdown or any code fences
                        2. DO NOT enclose your entire response in code fences
                        3. Do not use any opening or closing fences in your response
                        """
                    else:  # Continuation chunks
                        doc_template = f"""Create the NEXT PART (part {chunk_start//chunk_size + 1}) of an extremely detailed, comprehensive markdown document about this topic.
            
            Please continue the document with ADDITIONAL SECTIONS covering:
            - Advanced concepts and developments
            - Modern applications and technologies  
            - Current trends and future directions
            - Challenges and limitations
            
            This is a CONTINUATION of a document, so do not include introductory material that would already be covered.
            
            Format with proper markdown headings (## for main sections, ### for subsections).
            
            EXTREMELY IMPORTANT:
            1. DO NOT start your response with ```markdown or any code fences
            2. DO NOT enclose your entire response in code fences
            3. Do not use any opening or closing fences in your response
            
            IMPORTANT INSTRUCTIONS FOR IMAGE PLACEMENT:
            - If images are provided, include placeholder markers like [INSERT_IMAGE_1_HERE], [INSERT_IMAGE_2_HERE], etc.
            - Place these placeholders at meaningful points throughout your response, not all at the beginning
            - Good places for images are after introductory paragraphs or to illustrate key concepts
            - Do not cluster all images together - spread them throughout different sections."""
                elif format == 'html':
                    doc_template = f"""Create part {chunk_start//chunk_size + 1} of a comprehensive HTML document about this topic. The ENTIRE content must use proper HTML tags, not Markdown.

EXTREMELY IMPORTANT: You MUST use HTML tags for EVERYTHING and NEVER use Markdown syntax anywhere in your response.
For example:
- CORRECT: <h1>Main Heading</h1>
- INCORRECT: # Main Heading

- CORRECT: <h2>Section Heading</h2>
- INCORRECT: ## Section Heading

- CORRECT: <p>This is a paragraph with <strong>bold text</strong>.</p>
- INCORRECT: This is a paragraph with **bold text**.

- CORRECT: <ul><li>List item</li><li>Another item</li></ul>
- INCORRECT: - List item
             - Another item

DO NOT EVER USE # FOR HEADINGS OR ** FOR BOLD TEXT OR - FOR LISTS. Always use proper HTML tags like <h1>, <strong>, <ul><li>, etc.

Every single piece of content must be enclosed in appropriate HTML tags. Do not mix HTML and Markdown syntax anywhere.
                    
                    IMPORTANT INSTRUCTIONS FOR IMAGE PLACEMENT:
                    - If images are provided, include placeholder markers like [INSERT_IMAGE_1_HERE], [INSERT_IMAGE_2_HERE], etc.
                    - These simple placeholders will be replaced with actual images
                    - Place these image placeholders at meaningful points throughout your document
                    - Good places for images are after introductory paragraphs or to illustrate key concepts
                    - Do not cluster all images together - spread them throughout different sections
                    """
                else:
                    doc_template = f"""Create part {chunk_start//chunk_size + 1} of a comprehensive document about this topic in plain text format.
                    
            IMPORTANT INSTRUCTIONS FOR IMAGE PLACEMENT:
            - If images are provided, include placeholder markers like IMAGE_{chunk_start + 1}, IMAGE_{chunk_start + 2}, etc.
            - Place these markers at meaningful points throughout your document
            - Do not cluster all images together - spread them throughout different sections."""
                
                # Build the chunk-specific prompt
                prompt = f"""
                {doc_template}
                
                Topic: {query_str}
                
                Based on the following web research results, create a well-structured document section.
                
                RESEARCH DATA:
                {sources_info}
                
                Make this section detailed, informative, and comprehensive.
                """
                
                console.print(f"üß† Analyzing chunk {chunk_start//chunk_size + 1} of {(len(all_extracted_data) + chunk_size - 1)//chunk_size}...")
                
                # Get the LLM instance
                llm = get_llm()
                
                try:
                    # Generate content for this chunk
                    professional_mode = write  # Use professional mode when generating a document
                    chunk_response = asyncio.run(llm.generate_response(prompt, professional_mode=professional_mode))
                    chunked_responses.append(chunk_response)
                except Exception as e:
                    console.print(f"‚ùå Error generating response for chunk {chunk_start//chunk_size + 1}: {str(e)}")
                    continue
            
            # Combine all chunk responses into a single document
            if chunked_responses:
                response = "\n\n".join(chunked_responses)
                
                # For markdown, ensure we have a good title and table of contents
                if format == 'markdown' and not response.strip().startswith("# "):
                    title = query_str.title()
                    response = f"# {title}\n\n{response}"
                
                # Clean up any stray markdown code fences - import if needed
                if format == 'markdown':
                    from cliche.utils.generate_from_scrape import clean_markdown_document
                    response = clean_markdown_document(response)
                
                # Clean up any stray markdown code fences from HTML content
                if format == 'html':
                    # Remove any markdown code fences that might appear in the HTML content
                    response = re.sub(r'```html\s*', '', response)
                    response = re.sub(r'```\s*', '', response)
                    
                    # Convert headings (all levels h1-h6)
                    response = re.sub(r'^#{1}\s+(.+?)$', r'<h1>\1</h1>', response, flags=re.MULTILINE)
                    response = re.sub(r'^#{2}\s+(.+?)$', r'<h2>\1</h2>', response, flags=re.MULTILINE)
                    response = re.sub(r'^#{3}\s+(.+?)$', r'<h3>\1</h3>', response, flags=re.MULTILINE)
                    response = re.sub(r'^#{4}\s+(.+?)$', r'<h4>\1</h4>', response, flags=re.MULTILINE)
                    response = re.sub(r'^#{5}\s+(.+?)$', r'<h5>\1</h5>', response, flags=re.MULTILINE)
                    response = re.sub(r'^#{6}\s+(.+?)$', r'<h6>\1</h6>', response, flags=re.MULTILINE)
                    
                    # Convert bold text
                    response = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', response)
                    
                    # Convert italic text
                    response = re.sub(r'\*(.+?)\*', r'<em>\1</em>', response)
                    
                    # Convert list items
                    response = re.sub(r'^\s*-\s+(.+?)$', r'<li>\1</li>', response, flags=re.MULTILINE)
                    
                    # Wrap adjacent list items in <ul> tags
                    list_pattern = r'(<li>.*?</li>\s*){2,}'
                    matches = re.finditer(list_pattern, response, re.DOTALL)
                    for match in matches:
                        orig = match.group(0)
                        wrapped = f'<ul>{orig}</ul>'
                        response = response.replace(orig, wrapped)
                    
                    # Ensure all paragraphs are wrapped in <p> tags
                    # Find text that's not inside any HTML tags and wrap it with <p>
                    lines = response.split('\n')
                    for i, line in enumerate(lines):
                        if line.strip() and not re.match(r'^\s*<', line) and not re.match(r'^\s*$', line):
                            # If it's not already in an HTML tag, wrap it
                            lines[i] = f'<p>{line}</p>'
                    response = '\n'.join(lines)
            else:
                console.print("‚ùå No content could be generated from any chunks.")
                return 1  # Return error code
        
        # Process the generated content to replace image placeholders
        if image_data["images"] and (format == 'markdown' or format == 'html'):
            console.print(f"üñºÔ∏è Processing {len(image_data['images'])} images for document")
            
            # Check if any IMAGE_ placeholders are in the document
            placeholders_found = len(re.findall(r'\bIMAGE_\d+\b', response))
            
            # If no placeholders found, use AI-powered image placement
            if placeholders_found == 0:
                console.print("üí° Using AI-powered image placement to enhance the document...")
                
                # Split response into paragraphs for placement
                paragraphs = response.split('\n\n')
                
                # Get LLM recommendations for image placement
                insertion_points = None
                try:
                    insertion_points = asyncio.run(get_image_placement_suggestions(
                        llm=llm, 
                        document_content=response, 
                        image_count=len(image_data["images"]),
                        topic=query_str,
                        format=format
                    ))
                except Exception as e:
                    console.print(f"‚ö†Ô∏è Error getting image placement suggestions: {str(e)}")
                    insertion_points = None
                
                # If LLM suggestions aren't available or valid, fallback to our distribution method
                if not insertion_points:
                    # Find headings to identify section breaks
                    heading_indices = [i for i, p in enumerate(paragraphs) if p.startswith('#')]
                    
                    # If we have enough headings, distribute images after headings
                    if len(heading_indices) >= len(image_data["images"]):
                        # Choose evenly spaced heading indices
                        step = len(heading_indices) // (len(image_data["images"]) + 1)
                        if step < 1:
                            step = 1
                        
                        insertion_points = []
                        for i in range(1, len(image_data["images"]) + 1):
                            idx = min(i * step, len(heading_indices) - 1)
                            heading_idx = heading_indices[idx]
                            insertion_point = min(heading_idx + 1, len(paragraphs) - 1)
                            if insertion_point not in insertion_points:
                                insertion_points.append(insertion_point)
                    else:
                        # Not enough headings, distribute evenly throughout document
                        total_paragraphs = len(paragraphs)
                        spacing = total_paragraphs // (len(image_data["images"]) + 1)
                        
                        # Ensure we don't insert at the beginning
                        start_point = min(4, total_paragraphs // 10)
                        
                        insertion_points = []
                        for i in range(len(image_data["images"])):
                            # Calculate position ensuring even distribution
                            pos = start_point + (i + 1) * spacing
                            pos = min(pos, total_paragraphs - 1)
                            
                            # Avoid inserting before headings
                            if pos < total_paragraphs - 1 and paragraphs[pos + 1].startswith('#'):
                                pos += 2
                            
                            if pos not in insertion_points and pos < total_paragraphs:
                                insertion_points.append(pos)
                
                # Sort insertion points
                if insertion_points:
                    insertion_points.sort()
                    
                    # Make sure we don't have more insertion points than images
                    insertion_points = insertion_points[:len(image_data["images"])]
                    
                    # Insert images at the chosen points
                    for i, insertion_idx in enumerate(insertion_points):
                        if i < len(image_data["images"]):
                            img_data = image_data["images"][i]
                            img_alt = img_data["alt_text"] or "Image"
                            
                            if format == 'markdown':
                                img_content = f"\n\n![{img_alt}]({img_data['url']})\n\n"
                            else:  # HTML format
                                img_content = f"\n\n<img src=\"{img_data['url']}\" alt=\"{img_alt}\" style=\"max-width: 100%; height: auto;\">\n\n"
                                
                            paragraphs.insert(insertion_idx + i, img_content)
                    
                    # Reconstruct the document
                    response = '\n\n'.join(paragraphs)
            else:
                # Handle explicit placeholders
                console.print(f"üîÑ Processing {placeholders_found} image placeholders...")
                
                # Replace placeholders with actual images
                for i, img_data in enumerate(image_data["images"]):
                    img_idx = i + 1
                    if img_idx <= placeholders_found:
                        placeholder = f"IMAGE_{img_idx}"
                        if format == 'markdown':
                            img_content = f"![{img_data['alt_text'] or 'Image'}]({img_data['url']})"
                        else:  # HTML
                            img_content = f"<img src=\"{img_data['url']}\" alt=\"{img_data['alt_text'] or 'Image'}\" style=\"max-width: 100%; height: auto;\">"
                        
                        response = response.replace(placeholder, img_content)
            
            # Add credits at the end of the document
            if image_data["credits"]:
                if format == 'markdown':
                    response += "\n\n---\n\n## Image Credits\n\n"
                    for credit in image_data["credits"]:
                        response += f"* {credit}\n"
                else:  # HTML
                    response += "\n\n<hr>\n<h2>Image Credits</h2>\n<ul>\n"
                    for credit in image_data["credits"]:
                        response += f"<li>{credit}</li>\n"
                    response += "</ul>\n"
        
        # Determine what to do with the response based on write flag
        if write:
            # Get format-specific extension
            if format == 'markdown':
                ext = '.md'
            elif format == 'html':
                ext = '.html'
            else:
                ext = '.txt'
            
            # Generate default filename if none provided
            if not filename:
                # Create a filename from the first few words of the query
                words = query_str.lower().split()[:3]
                base_filename = 'research_' + '_'.join(words) + ext
                # Use the docs/research directory for organization
                output_dir = get_docs_dir('research')
                # Get a unique filename
                unique_filename = get_unique_filename(output_dir, base_filename)
                file_path = str(output_dir / unique_filename)
            else:
                # Make sure filename has correct extension
                if not any(filename.endswith(e) for e in ['.txt', '.md', '.html']):
                    filename += ext
                
                # Use specified filename, but ensure it's in the right directory
                if os.path.dirname(filename):
                    # If a full path is provided, use it
                    file_path = filename
                else:
                    # Otherwise, put it in the docs/research directory
                    output_dir = get_docs_dir('research')
                    # Get a unique filename
                    unique_filename = get_unique_filename(output_dir, filename)
                    file_path = str(output_dir / unique_filename)
            
            # Ensure HTML has proper structure
            if format == 'html' and not response.strip().startswith('<!DOCTYPE html>'):
                response = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{query_str} - Research</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }}
        img {{ max-width: 100%; height: auto; display: block; margin: 20px 0; }}
        code {{ background-color: #f5f5f5; padding: 2px 4px; border-radius: 3px; }}
        pre {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        h1, h2, h3 {{ color: #333; }}
        .unsplash-image {{ border-radius: 5px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
    </style>
</head>
<body>
{response}
</body>
</html>"""
            
            # Save to file
            save_text_to_file(response, file_path)
            console.print(f"‚úÖ Research document saved to: {file_path}")
        else:
            # Display the response in terminal
            console.print("\n" + "=" * 60)
            console.print("üìö RESEARCH RESULTS")
            console.print("=" * 60)
            console.print(f"\nüí° {response}\n")
            console.print("=" * 60)
            console.print(f"Sources: {len(extracted_data)} websites analyzed")
            console.print("=" * 60)
        
        return 0  # Return success code
        
    except AttributeError as e:
        console.print("‚ùå Error: Provider not properly configured")
        return 1  # Return error code
    except Exception as e:
        console.print(f"‚ùå Error generating response: {str(e)}")
        return 1  # Return error code

def brave_search(query, num_results=5, api_key=None):
    """Perform a search using the Brave Search API.
    
    Args:
        query: The search query
        num_results: Maximum number of results to return
        api_key: Brave Search API key (optional, will try to get from config)
        
    Returns:
        List of search result dictionaries
    """
    if not BRAVE_SEARCH_AVAILABLE:
        click.echo("‚ùå Requests package not installed for Brave Search. Run: pip install requests")
        return []
    
    # Get API key from config if not provided
    if not api_key:
        # Try to get from environment or config
        from ..core import CLIche
        try:
            cliche = CLIche()
            brave_config = cliche.config.config.get("services", {}).get("brave_search", {})
            api_key = brave_config.get("api_key")
        except Exception:
            api_key = os.getenv("BRAVE_SEARCH_API_KEY")
    
    if not api_key:
        click.echo("‚ö†Ô∏è Brave Search API key not configured. Skipping Brave Search.")
        return []
    
    try:
        # Brave Search API endpoint
        url = "https://api.search.brave.com/res/v1/web/search"
        
        # Request headers with API key
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "X-Subscription-Token": api_key
        }
        
        # Request parameters
        params = {
            "q": query,
            "count": min(num_results, 10),  # Brave Search API max is 10 per request
            "search_lang": "en"
        }
        
        # Make the request
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        
        # Extract results
        results = []
        for item in data.get("web", {}).get("results", []):
            results.append({
                'link': item.get('url', ''),
                'title': item.get('title', ''),
                'snippet': item.get('description', '')
            })
        
        return results
        
    except Exception as e:
        click.echo(f"Error during Brave Search: {str(e)}")
        return []

async def get_image_placement_suggestions(llm, document_content, image_count, topic, format):
    """Ask the LLM to suggest optimal image placement locations in the document.
    
    Args:
        llm: The LLM instance to use
        document_content: The content of the document to analyze
        image_count: Number of images to place
        topic: The document topic
        format: The document format (markdown, html)
        
    Returns:
        A list of suggested paragraph indices where images should be placed
    """
    # Create a prompt specifically for image placement
    placement_prompt = f"""
    I've generated a {format} document about "{topic}". Now I need to place {image_count} images at optimal locations.
    
    Please analyze this document and suggest the {image_count} best locations to place relevant images.
    
    For each suggested location:
    1. Identify the paragraph number (counting from 0)
    2. Explain why this is a good location (e.g., introduces a key concept, visualizes an example)
    
    Your response should be in this format:
    PLACEMENT 1: Paragraph X - Reason
    PLACEMENT 2: Paragraph Y - Reason
    ... and so on
    
    Here's the document content:
    ---
    {document_content}
    ---
    
    IMPORTANT: Focus on finding contextually relevant placements where images would enhance understanding.
    """
    
    # Get suggestions from the LLM
    response = await llm.generate_response(placement_prompt, professional_mode=True)
    
    # Parse the response to extract paragraph indices
    suggested_indices = []
    
    # Look for "PLACEMENT X: Paragraph Y" patterns
    placement_pattern = r'PLACEMENT\s+\d+\s*:\s*Paragraph\s+(\d+)'
    matches = re.finditer(placement_pattern, response)
    
    for match in matches:
        try:
            paragraph_index = int(match.group(1))
            suggested_indices.append(paragraph_index)
        except:
            continue
    
    # If we couldn't extract valid suggestions, fallback to evenly distributed placements
    if not suggested_indices or len(suggested_indices) < image_count:
        # Our existing fallback method which evenly distributes images
        return None
    
    # Return only up to the requested number of placements
    return suggested_indices[:image_count]

if __name__ == "__main__":
    research()
